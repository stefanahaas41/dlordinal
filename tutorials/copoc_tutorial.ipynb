{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2702f22b2d8b589",
   "metadata": {},
   "source": "## Import libraries"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T09:31:59.062412Z",
     "start_time": "2025-09-17T09:31:50.710587Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from skorch import NeuralNetClassifier\n",
    "from torch import cuda, nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torchvision import models\n",
    "from torchvision.transforms.v2 import Compose, ToDtype, ToImage\n",
    "\n",
    "from dlordinal.datasets import FGNet\n",
    "from dlordinal.metrics import accuracy_off1, amae, mmae, ranked_probability_score\n",
    "from dlordinal.output_layers.copoc import COPOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c97d9a3dbf3ff",
   "metadata": {},
   "source": "## Import FGNet dataset"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63b68abab6465cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T09:31:59.482129Z",
     "start_time": "2025-09-17T09:31:59.412290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n",
      "Files already downloaded and verified\n",
      "Files already processed and verified\n",
      "Files already split and verified\n"
     ]
    }
   ],
   "source": [
    "fgnet_train = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=Compose([ToImage(), ToDtype(torch.float32, scale=True)]),\n",
    ")\n",
    "\n",
    "fgnet_test = FGNet(\n",
    "    root=\"./datasets\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=Compose([ToImage(), ToDtype(torch.float32, scale=True)]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d6a8141678f1b",
   "metadata": {},
   "source": "## Model training"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848ea2b21d4c3708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T09:39:14.779880Z",
     "start_time": "2025-09-17T09:31:59.531082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001B[36m1.7528\u001B[0m       \u001B[32m0.1925\u001B[0m        \u001B[35m1.8009\u001B[0m  11.5622\n",
      "      2        \u001B[36m1.7169\u001B[0m       \u001B[32m0.2360\u001B[0m        \u001B[35m1.7633\u001B[0m  14.0602\n",
      "      3        \u001B[36m1.6682\u001B[0m       0.1615        1.8305  10.6949\n",
      "      4        \u001B[36m1.6394\u001B[0m       0.2360        1.7851  10.9021\n",
      "      5        \u001B[36m1.6377\u001B[0m       \u001B[32m0.2671\u001B[0m        \u001B[35m1.7353\u001B[0m  19.3132\n",
      "      6        \u001B[36m1.5851\u001B[0m       0.1366        1.8213  14.1549\n",
      "      7        \u001B[36m1.5634\u001B[0m       \u001B[32m0.3913\u001B[0m        \u001B[35m1.6269\u001B[0m  12.9707\n",
      "      8        \u001B[36m1.5358\u001B[0m       0.3043        1.7113  12.1856\n",
      "      9        \u001B[36m1.4657\u001B[0m       0.3727        1.6548  13.1590\n",
      "     10        1.4998       0.3602        1.6590  12.9906\n",
      "     11        \u001B[36m1.4155\u001B[0m       0.3106        1.7164  17.4253\n",
      "     12        \u001B[36m1.3896\u001B[0m       \u001B[32m0.4161\u001B[0m        \u001B[35m1.6075\u001B[0m  24.2347\n",
      "     13        \u001B[36m1.3815\u001B[0m       0.3168        1.7024  21.1969\n",
      "     14        \u001B[36m1.3375\u001B[0m       0.3851        1.6380  26.3443\n",
      "     15        \u001B[36m1.3006\u001B[0m       \u001B[32m0.4286\u001B[0m        \u001B[35m1.5967\u001B[0m  11.4351\n",
      "     16        \u001B[36m1.2869\u001B[0m       \u001B[32m0.4845\u001B[0m        \u001B[35m1.5264\u001B[0m  10.8281\n",
      "     17        \u001B[36m1.2371\u001B[0m       0.4596        1.5640  14.5840\n",
      "     18        \u001B[36m1.2207\u001B[0m       0.4658        1.5605  12.8186\n",
      "     19        \u001B[36m1.1941\u001B[0m       0.4472        1.6103  12.9709\n",
      "     20        \u001B[36m1.1889\u001B[0m       0.4783        1.5679  15.1876\n",
      "     21        \u001B[36m1.1791\u001B[0m       0.4348        1.6183  13.5750\n",
      "     22        \u001B[36m1.1500\u001B[0m       \u001B[32m0.5652\u001B[0m        \u001B[35m1.4881\u001B[0m  13.5880\n",
      "     23        \u001B[36m1.1477\u001B[0m       0.5590        \u001B[35m1.4725\u001B[0m  12.6284\n",
      "     24        \u001B[36m1.1346\u001B[0m       \u001B[32m0.5963\u001B[0m        \u001B[35m1.4421\u001B[0m  13.4392\n",
      "     25        \u001B[36m1.1267\u001B[0m       \u001B[32m0.6273\u001B[0m        \u001B[35m1.4262\u001B[0m  11.8076\n",
      "     26        \u001B[36m1.1221\u001B[0m       0.6149        1.4400  13.4761\n",
      "     27        \u001B[36m1.1178\u001B[0m       0.5528        1.4693  13.1400\n",
      "     28        \u001B[36m1.1143\u001B[0m       0.5901        1.4667  13.4669\n",
      "     29        \u001B[36m1.1113\u001B[0m       0.5714        1.4719  17.4403\n",
      "     30        \u001B[36m1.1093\u001B[0m       0.5714        1.4707  13.4026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=6, bias=True)\n",
       "      (1): COPOC()\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "num_classes = len(fgnet_train.classes)\n",
    "\n",
    "# Initialize ResNet18 model\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "# Add COPOC layer\n",
    "model.fc = nn.Sequential(nn.Linear(model.fc.in_features, num_classes), COPOC())\n",
    "model = model.to(device)\n",
    "\n",
    "# Skorch estimator\n",
    "estimator = NeuralNetClassifier(\n",
    "    module=model,\n",
    "    criterion=CrossEntropyLoss().to(device),\n",
    "    optimizer=Adam,\n",
    "    lr=0.001,\n",
    "    max_epochs=30,\n",
    "    device=device,\n",
    "    batch_size=200,\n",
    ")\n",
    "\n",
    "# Prepare training labels\n",
    "y_train = torch.tensor(fgnet_train.targets, dtype=torch.long)\n",
    "\n",
    "# Train model\n",
    "estimator.fit(fgnet_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca28be97f240d4",
   "metadata": {},
   "source": "## Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b8798ece302022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T09:39:16.150888Z",
     "start_time": "2025-09-17T09:39:14.811577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unimodal predictions: 201 / 201 (100.00%)\n",
      "ACC: 0.5422885572139303\n",
      "1OFF: 0.9353233830845771\n",
      "MAE: 0.527363184079602\n",
      "QWK: 0.8252978168618028\n",
      "AMAE: 0.5437950937950937\n",
      "MMAE: 0.9285714285714286\n",
      "RPS: 0.6556535991962896\n",
      "Unimodality: 1.0\n",
      "[[18  3  1  0  0  0]\n",
      " [ 7 31 16  5  1  0]\n",
      " [ 0  6 18  9  0  0]\n",
      " [ 0  1 12 24  5  0]\n",
      " [ 0  0  3 10 15  2]\n",
      " [ 0  0  0  2  9  3]]\n",
      "{'ACC': 0.5422885572139303, '1OFF': 0.9353233830845771, 'MAE': 0.527363184079602, 'QWK': 0.8252978168618028, 'AMAE': 0.5437950937950937, 'MMAE': 0.9285714285714286, 'RPS': 0.6556535991962896, 'Unimodality': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def is_unimodal(probs):\n",
    "    \"\"\"Check if a 1D array is unimodal (increases to a peak, then decreases).\"\"\"\n",
    "    peak_idx = np.argmax(probs)\n",
    "    # Increasing up to peak\n",
    "    inc = np.all(np.diff(probs[: peak_idx + 1]) >= 0)\n",
    "    # Decreasing after peak\n",
    "    dec = np.all(np.diff(probs[peak_idx:]) <= 0)\n",
    "    return inc and dec\n",
    "\n",
    "\n",
    "def check_unimodality(y_pred):\n",
    "    \"\"\"Check unimodality for each row in y_pred and return the proportion.\"\"\"\n",
    "    unimodal_flags = np.array([is_unimodal(row) for row in y_pred])\n",
    "    # Proportion of rows that are unimodal\n",
    "    proportion = np.mean(unimodal_flags)\n",
    "    print(\n",
    "        f\"Unimodal predictions: {np.sum(unimodal_flags)} / {len(y_pred)} ({proportion})\"\n",
    "    )\n",
    "    return proportion\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate various metrics given true labels and predicted probabilities.\"\"\"\n",
    "    if np.allclose(np.sum(y_pred, axis=1), 1):\n",
    "        y_pred_proba = y_pred\n",
    "    else:\n",
    "        y_pred_proba = softmax(y_pred, axis=1)\n",
    "\n",
    "    y_pred_max = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Metrics\n",
    "    amae_metric = amae(y_true, y_pred)\n",
    "    mmae_metric = mmae(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred_max)\n",
    "    acc = accuracy_score(y_true, y_pred_max)\n",
    "    acc_1off = accuracy_off1(y_true, y_pred)\n",
    "    qwk = cohen_kappa_score(y_true, y_pred_max, weights=\"quadratic\")\n",
    "    rps = ranked_probability_score(y_true, y_pred_proba)\n",
    "    # Check unimodality\n",
    "    unimodal_prop = check_unimodality(y_pred_proba)\n",
    "\n",
    "    metrics = {\n",
    "        \"ACC\": acc,\n",
    "        \"1OFF\": acc_1off,\n",
    "        \"MAE\": mae,\n",
    "        \"QWK\": qwk,\n",
    "        \"AMAE\": amae_metric,\n",
    "        \"MMAE\": mmae_metric,\n",
    "        \"RPS\": rps,\n",
    "        \"Unimodality\": unimodal_prop,\n",
    "    }\n",
    "\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(confusion_matrix(y_true, y_pred_max))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "test_probs = estimator.predict_proba(fgnet_test)\n",
    "print(calculate_metrics(fgnet_test.targets, test_probs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
